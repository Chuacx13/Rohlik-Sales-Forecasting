{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from datetime import timedelta\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv('data/calendar.csv')\n",
    "inventory_df = pd.read_csv('data/inventory.csv')\n",
    "sales_test_df = pd.read_csv('data/sales_test.csv')\n",
    "sales_train_df = pd.read_csv('data/sales_train.csv')\n",
    "solution_df = pd.read_csv('data/solution.csv')\n",
    "test_weights_df = pd.read_csv('data/test_weights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_df = sales_train_df.drop(columns=['availability'])\n",
    "sales_train_df.dropna(subset=['sales'], inplace=True)\n",
    "sales_test_df['sales'] = 0\n",
    "df = pd.concat([sales_train_df, sales_test_df], ignore_index=True).sort_values('date')\n",
    "df = df.merge(calendar_df, on=['date', 'warehouse'], how='left')\n",
    "df = df.merge(inventory_df, on=['unique_id', 'warehouse'], how='left')\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [1,2,3,4,5,6,7,14,21,28,60,90,120,180,270,360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(df):\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['year_sin'] = np.sin((df['year'] - df['year'].min()) / (df['year'].max() - df['year'].min()) * 2 * np.pi)\n",
    "    df['month_sin'] = np.sin(df['month'] / 12 * 2 * np.pi)\n",
    "    df['year_cos'] = np.cos((df['year'] - df['year'].min()) / (df['year'].max() - df['year'].min()) * 2 * np.pi)\n",
    "    df['month_cos'] = np.cos(df['month'] / 12 * 2 * np.pi)\n",
    "    return df\n",
    "\n",
    "def add_product_cat(df):\n",
    "    df['category'] = df['name'].str.split('_', expand=True)[0]\n",
    "    return df\n",
    "\n",
    "def add_lagged_product_sales(df):\n",
    "    for period in periods:\n",
    "        df[f'sales_{period}'] = df.groupby(['warehouse', 'name'])['sales'].shift(periods=period)\n",
    "    return df \n",
    "\n",
    "def add_order_difference(df):\n",
    "    for period in periods:\n",
    "        df[f'orders_{period}'] = df.groupby(['warehouse', 'name'])['total_orders'].shift(periods=period)\n",
    "        df[f'orders_diff_{period}'] = df['total_orders'] - df[f'orders_{period}']\n",
    "    return df\n",
    "\n",
    "def add_country_and_subcountry(df):\n",
    "    warehouse_to_country = {\n",
    "        'Budapest_1': 'Hungary',\n",
    "        'Prague_1': 'Czech Republic',\n",
    "        'Prague_2': 'Czech Republic',\n",
    "        'Prague_3': 'Czech Republic',\n",
    "        'Brno_1': 'Czech Republic',\n",
    "        'Munich_1': 'Germany',\n",
    "        'Frankfurt_1': 'Germany'\n",
    "    }\n",
    "\n",
    "    warehouse_to_subcountry = {\n",
    "        'Budapest_1': 'Hungary',\n",
    "        'Prague_1': 'Czech Republic 1',\n",
    "        'Prague_2': 'Czech Republic 2',\n",
    "        'Prague_3': 'Czech Republic 2',\n",
    "        'Brno_1': 'Czech Republic 1',\n",
    "        'Munich_1': 'Germany 1',\n",
    "        'Frankfurt_1': 'Germany 2'\n",
    "    }\n",
    "\n",
    "    # Add a country column using the mapping\n",
    "    df['country'] = df['warehouse'].map(warehouse_to_country)\n",
    "    \n",
    "    # Add a country column using the mapping\n",
    "    df['subcountry'] = df['warehouse'].map(warehouse_to_subcountry)\n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def add_discount_features(df):\n",
    "    df['best_discount'] = df[['type_0_discount', 'type_1_discount','type_2_discount', 'type_3_discount', 'type_4_discount', 'type_5_discount', 'type_6_discount']].max(axis=1)\n",
    "\n",
    "    df['type_0_discount_subcountry'] = df['subcountry']\n",
    "    df['type_1_discount_subcountry'] = df['subcountry']\n",
    "    df['type_2_discount_subcountry'] = df['subcountry']\n",
    "    df['type_3_discount_subcountry'] = df['subcountry']\n",
    "    df['type_4_discount_subcountry'] = df['subcountry']\n",
    "    df['type_5_discount_subcountry'] = df['subcountry']\n",
    "    df['type_6_discount_subcountry'] = df['subcountry']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_processed_holiday(df):\n",
    "    holiday_mappings = {\n",
    "        'Hungary': [\n",
    "            'Good Friday',\n",
    "            'Easter Monday',\n",
    "            'Whit Monday',\n",
    "            'Whit Sunday'\n",
    "            'Christmas Eve',\n",
    "            '1st Christmas Day',\n",
    "            '2nd Christmas Day',\n",
    "            'New Years Day',\n",
    "            'Assumption of the Virgin Mary'\n",
    "        ],\n",
    "        'Czech Republic': [\n",
    "            'Good Friday',\n",
    "            'Easter Monday',\n",
    "            'Whit Sunday',\n",
    "            'Whit Monday',\n",
    "            'Christmas Eve',\n",
    "            '1st Christmas Day',\n",
    "            '2nd Christmas Day',\n",
    "            'New Years Day',\n",
    "            'Epiphany',\n",
    "            'Cyrila a Metodej'\n",
    "        ],\n",
    "        'Germany': [\n",
    "            'Good Friday',\n",
    "            'Easter Monday',\n",
    "            'Corpus Christi',\n",
    "            'Whit Sunday',\n",
    "            'Whit Monday',\n",
    "            'Christmas Eve',\n",
    "            '1st Christmas Day',\n",
    "            '2nd Christmas Day',\n",
    "            'New Years Day',\n",
    "            'Ascension day',\n",
    "            'Assumption of the Virgin Mary',\n",
    "            'Reformation Day'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df['holiday_name'] = df['holiday_name'].fillna('No Holiday')\n",
    "\n",
    "    df['holiday'] = df.apply(\n",
    "        lambda row: 1 if row['country'] in holiday_mappings and row['holiday_name'] in holiday_mappings[row['country']] else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Get rows where holiday == 1\n",
    "    holiday_rows = df[df['holiday'] == 1].drop_duplicates(subset='date', ignore_index=True)\n",
    "    \n",
    "    # Set holidays in the range 3 days before and 1 day after, within the same subcountry\n",
    "    for _, row in holiday_rows.iterrows():\n",
    "        start_date = row['date'] - timedelta(days=3)\n",
    "        end_date = row['date'] + timedelta(days=1)\n",
    "        country = row['country']  # Get the country for the holiday row\n",
    "        \n",
    "        # Update rows in the date range and the same country\n",
    "        df.loc[\n",
    "            (df['date'] >= start_date) & \n",
    "            (df['date'] <= end_date) & \n",
    "            (df['country'] == country),\n",
    "            'holiday'\n",
    "        ] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_target_encoding(df):\n",
    "\n",
    "    df['target_l1_cat_trend'] = (\n",
    "        df\n",
    "        .groupby(['subcountry', 'L1_category_name_en'])['total_orders']\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "    df['target_l1_cat_daily'] = (\n",
    "        df.groupby(['date', 'subcountry', 'L1_category_name_en'])['total_orders']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    df['target_l2_cat_trend'] = (\n",
    "        df\n",
    "        .groupby(['subcountry', 'L2_category_name_en'])['total_orders']\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "    df['target_l2_cat_daily'] = (\n",
    "        df.groupby(['date', 'subcountry', 'L2_category_name_en'])['total_orders']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    df['target_l3_cat_trend'] = (\n",
    "        df\n",
    "        .groupby(['subcountry', 'L3_category_name_en'])['total_orders']\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "    df['target_l3_cat_daily'] = (\n",
    "        df.groupby(['date', 'subcountry', 'L3_category_name_en'])['total_orders']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    df['target_l4_cat_trend'] = (\n",
    "        df\n",
    "        .groupby(['subcountry', 'L4_category_name_en'])['total_orders']\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "    df['target_l4_cat_daily'] = (\n",
    "        df.groupby(['date', 'subcountry', 'L4_category_name_en'])['total_orders']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    df['target_category_trend'] = (\n",
    "        df\n",
    "        .groupby(['subcountry', 'category'])['total_orders']\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "    df['target_category_daily'] = (\n",
    "        df.groupby(['date', 'subcountry', 'category'])['total_orders']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_date_features(df)\n",
    "df = add_product_cat(df)\n",
    "df = add_lagged_product_sales(df)\n",
    "df = add_order_difference(df)\n",
    "df = add_country_and_subcountry(df)\n",
    "df = add_processed_holiday(df)\n",
    "df = add_discount_features(df)\n",
    "df = add_target_encoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "df[(df.name=='Croissant_36')&(df.warehouse==\"Brno_1\")][['warehouse','name','sales','sales_14', 'type_0_discount_subcountry', 'country', 'subcountry', 'orders_diff_14', 'orders_14', 'target_category_trend']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensuring correct datatypes\n",
    "for col in df.select_dtypes(\"object\").columns:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_train_df['date'].min())\n",
    "print(sales_train_df['date'].max())\n",
    "print(sales_test_df['date'].min())\n",
    "print(sales_test_df['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_date  = '2020-08-01'\n",
    "train_end_date  = '2024-06-02'\n",
    "\n",
    "train = df[(df.index >= train_start_date) & (df.index <= train_end_date)]\n",
    "test  = df[(df.index >  train_end_date)]\n",
    "\n",
    "X_train = train.drop(['sales'], axis=1)\n",
    "y_train = train['sales']\n",
    "\n",
    "X_test = test.drop(['sales'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameter from here: https://www.kaggle.com/code/meryentr/rohlik-sales-lightgbm-lb-20-75\n",
    "\n",
    "lightgbm_params={ \n",
    "    'learning_rate': 0.021796506746095975,\n",
    "    'num_leaves': 93,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 25,\n",
    "    'subsample': 0.7057135664023435,\n",
    "    'colsample_bytree': 0.8528497905459008,\n",
    "    'reg_alpha': 0.036786449788597686,\n",
    "    'reg_lambda': 0.3151110021900479,\n",
    "    'num_boost_round': 9800,\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': 0.021796506746095975,  \n",
    "    'max_leaves': 93,\n",
    "    'n_estimators': 9800,  \n",
    "    'max_depth': 10,  \n",
    "    'min_child_weight': 25, \n",
    "    'subsample': 0.7057135664023435,  \n",
    "    'colsample_bytree': 0.8528497905459008,  \n",
    "    'reg_alpha': 0.036786449788597686, \n",
    "    'reg_lambda': 0.3151110021900479,  \n",
    "    'objective': 'reg:squarederror',  \n",
    "    'eval_metric': 'mae', \n",
    "    'tree_method': 'hist', \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main LightGBM Model\n",
    "main_lightgbm_model = lgb.LGBMRegressor(**lightgbm_params)\n",
    "main_lightgbm_model.fit(X_train, y_train)\n",
    "main_lightgbm_model.booster_.save_model(\"main_lightgbm_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({'name':main_lightgbm_model.feature_name_})\n",
    "feature_importance_df['importance'] = main_lightgbm_model.feature_importances_\n",
    "feature_importance_df['group'] = feature_importance_df['name'].apply(\n",
    "    lambda x: 'lagged sales features' if 'sales_' in x else 'other features')\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "ax = sns.barplot(data=feature_importance_df, x='importance', y='name',  hue='group', dodge=False)\n",
    "ax.set_title(f\"Feature importances\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_country_features = feature_importance_df.iloc[:20]['name'].tolist()\n",
    "print(recursive_country_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_forecast_lgbm(country, X_test, X_train, unique_dates, train, recursive_country_features):\n",
    "    results = []\n",
    "    X_test_copy = X_test.copy()\n",
    "\n",
    "    curr_X_train = X_train[X_train.country == country][recursive_country_features]\n",
    "    curr_y_train = train[train.country == country]['sales']\n",
    "    curr_X_test = X_test_copy[X_test_copy.country == country][recursive_country_features]\n",
    "\n",
    "    for next_date in unique_dates:\n",
    "        model = lgb.LGBMRegressor(**lightgbm_params)\n",
    "        model.fit(curr_X_train, curr_y_train)\n",
    "\n",
    "        # Predict for current date\n",
    "        curr_X = curr_X_test.loc[next_date]\n",
    "        pred_sales = model.predict(curr_X)\n",
    "\n",
    "        # Create new data to train the model with\n",
    "        new_X = curr_X.copy()\n",
    "        pred_sales[pred_sales < 0] = 0\n",
    "        results.append(pd.Series(pred_sales, index=curr_X.index))\n",
    "        new_y = pd.Series(pred_sales, index=curr_X.index, name='sales')\n",
    "\n",
    "        # Add new data back to original data\n",
    "        curr_X_train = pd.concat([curr_X_train, new_X], ignore_index=False)\n",
    "        curr_y_train = pd.concat([curr_y_train, new_y], ignore_index=False)\n",
    "\n",
    "    forecast_df = pd.concat(results, axis=0).to_frame(name=f'{country}_pred_sales')\n",
    "    curr_X_test = X_test_copy[X_test_copy.country == country] # Redefine current X test to merge\n",
    "    curr_X_test['id'] = curr_X_test['unique_id'].astype(str) + \"_\" + curr_X_test.index.astype(str)\n",
    "\n",
    "    merged_df = pd.concat([curr_X_test , forecast_df], axis=1)[['id', f'{country}_pred_sales']]\n",
    "\n",
    "    model.booster_.save_model(f\"lightgbm_model_{country}_rec.json\")\n",
    "    merged_df.to_csv(f'{country}_rec_predictions.csv')\n",
    "\n",
    "    return model, merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_performance(y_pred):\n",
    "    print(f\"Mean of y_pred: {np.mean(y_pred):.4f}\")\n",
    "    print(f\"Variance of y_pred: {np.var(y_pred):.4f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = X_train['country'].unique()\n",
    "unique_dates = X_test.index.unique()\n",
    "country_rec_preds = {}\n",
    "country_rec_model = {}\n",
    "\n",
    "for country in countries:\n",
    "    curr_model, curr_y_pred = recursive_forecast_lgbm(country, X_test, X_train, unique_dates, train, recursive_country_features)\n",
    "    country_rec_preds[country] = curr_y_pred\n",
    "    country_rec_model[country] = curr_model\n",
    "    check_model_performance(curr_y_pred[f'{country}_pred_sales'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_features = feature_importance_df.iloc[:30]['name'].tolist()\n",
    "print(xgboost_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main XGB Model\n",
    "main_xgb_model = xgb.XGBRegressor(**xgb_params, enable_categorical=True)\n",
    "main_xgb_model.fit(X_train[xgboost_features], y_train)\n",
    "main_xgb_model.save_model(\"main_xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train one model per country\n",
    "country_models = {}\n",
    "\n",
    "countries = X_train['country'].unique()\n",
    "for country in countries:\n",
    "    curr_X_train = X_train[X_train.country == country]\n",
    "    curr_y_train = train[train.country == country]['sales']\n",
    "    model = lgb.LGBMRegressor(**lightgbm_params)\n",
    "    model.fit(curr_X_train, curr_y_train)\n",
    "    country_models[country] = model\n",
    "    model.booster_.save_model(f\"lightgbm_model_{country}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Solution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_copy = X_test.copy()\n",
    "X_test_copy['id'] = X_test_copy['unique_id'].astype(str) + \"_\" + X_test_copy.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_lightgbm_y_pred = main_lightgbm_model.predict(X_test)\n",
    "main_lightgbm_y_pred[main_lightgbm_y_pred<0] = 0\n",
    "\n",
    "main_lightgbm_y_pred_df = pd.DataFrame(main_lightgbm_y_pred, columns=['Main_LGBM_pred'], index=X_test_copy.index)\n",
    "main_lightgbm_df = pd.concat([X_test_copy['id'], main_lightgbm_y_pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_xgb_y_pred = main_xgb_model.predict(X_test[xgboost_features])\n",
    "main_xgb_y_pred[main_xgb_y_pred<0] = 0\n",
    "\n",
    "main_xgb_y_pred_df = pd.DataFrame(main_xgb_y_pred, columns=['Main_XGB_pred'], index=X_test_copy.index)\n",
    "main_xgb_df = pd.concat([X_test_copy['id'], main_xgb_y_pred_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_performance(main_lightgbm_y_pred)\n",
    "check_model_performance(main_xgb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_preds = {}\n",
    "for country, country_model in country_models.items():\n",
    "    curr_X_test = X_test[X_test.country == country]\n",
    "    curr_lightgbm_y_pred = country_model.predict(curr_X_test)\n",
    "    curr_lightgbm_y_pred[curr_lightgbm_y_pred<0] = 0  \n",
    "    print(country)\n",
    "    check_model_performance(curr_lightgbm_y_pred)  \n",
    "\n",
    "    curr_X_test['id'] = curr_X_test['unique_id'].astype(str) + \"_\" + curr_X_test.index.astype(str)\n",
    "    curr_lightgbm_y_pred_df = pd.DataFrame(curr_lightgbm_y_pred, columns=[f'{country}_pred'], index=curr_X_test.index)\n",
    "    curr_lightgbm_df = pd.concat([curr_X_test['id'], curr_lightgbm_y_pred_df], axis=1)\n",
    "    country_preds[country] = curr_lightgbm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_preds = {}\n",
    "countries = X_train['country'].unique()\n",
    "for country in countries:\n",
    "    country_model = lgb.Booster(model_file=f\"lightgbm_model_{country}.json\")\n",
    "    curr_X_test = X_test[X_test.country == country]\n",
    "    curr_lightgbm_y_pred = country_model.predict(curr_X_test)\n",
    "    curr_lightgbm_y_pred[curr_lightgbm_y_pred<0] = 0  \n",
    "    print(country)\n",
    "    check_model_performance(curr_lightgbm_y_pred)  \n",
    "    \n",
    "    curr_X_test['id'] = curr_X_test['unique_id'].astype(str) + \"_\" + curr_X_test.index.astype(str)\n",
    "    curr_lightgbm_y_pred_df = pd.DataFrame(curr_lightgbm_y_pred, columns=[f'{country}_pred'], index=curr_X_test.index)\n",
    "    curr_lightgbm_df = pd.concat([curr_X_test['id'], curr_lightgbm_y_pred_df], axis=1)\n",
    "    country_preds[country] = curr_lightgbm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring and Submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['id'] = test['unique_id'].astype(str) + \"_\" + test.index.astype(str)\n",
    "test = test[['id']]\n",
    "test = test.merge(main_lightgbm_df, on='id', how='left')\n",
    "test = test.merge(main_xgb_df, on='id', how='left')\n",
    "\n",
    "for country, country_pred in country_preds.items():\n",
    "    test = test.merge(country_pred, on='id', how='left')\n",
    "\n",
    "for country, country_rec_pred in country_rec_preds.items():\n",
    "    test = test.merge(country_rec_pred, on='id', how='left')   \n",
    "\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['id', 'Main_LGBM_pred', 'Main_XGB_pred', 'Czech Republic_pred',\n",
    "       'Hungary_pred', 'Germany_pred', 'Germany_pred_sales', \n",
    "       'Czech Republic_pred_sales', 'Hungary_pred_sales']] \n",
    "test['sales_hat'] = test.drop(columns=['id']).sum(axis=1, skipna=True) / 4\n",
    "test[['id','sales_hat']].to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
